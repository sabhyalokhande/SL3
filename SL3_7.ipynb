{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYrze9szB1q0",
        "outputId": "49ae8aaa-74d0-4ff5-f2ee-84a5bbb9336c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = r\"/content/text.txt\"  # Replace with your file path\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    document = file.read()\n",
        "print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptaVI2zNLjvo",
        "outputId": "45bc0846-15af-49d9-c376-033c6dda56ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'  Natural Language Processing is fun',\n",
            "        np.nan,\n",
            "        '   ',\n",
            "        'I love AI and machine learning',\n",
            "        '',\n",
            "        'Deep Learning is a subset of AI',\n",
            "        '  NLP applications are amazing    '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=nltk.sent_tokenize(document)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNn1rn-FMONe",
        "outputId": "f5ebd21b-e762-4c98-fe46-26942c00188a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'  Natural Language Processing is fun',\\n        np.nan,\\n        '   ',\\n        'I love AI and machine learning',\\n        '',\\n        'Deep Learning is a subset of AI',\\n        '  NLP applications are amazing    '\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_word=nltk.word_tokenize(document)\n",
        "print(token_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIAzerW7Mw9s",
        "outputId": "2586f03a-e1d3-44c5-fb83-19e3ba5b0c5f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'\", 'Natural', 'Language', 'Processing', 'is', 'fun', \"'\", ',', 'np.nan', ',', \"'\", \"'\", ',', \"'\", 'I', 'love', 'AI', 'and', 'machine', 'learning', \"'\", ',', '``', ',', \"'Deep\", 'Learning', 'is', 'a', 'subset', 'of', 'AI', \"'\", ',', \"'\", 'NLP', 'applications', 'are', 'amazing', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags=nltk.pos_tag(token_word)\n",
        "print(\"POS Tagging:\\n\",pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMP5oTjPMlck",
        "outputId": "52e4b9c8-6ffd-42c5-d4bb-049a6be45c5b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging:\n",
            " [(\"'\", 'POS'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('fun', 'NN'), (\"'\", \"''\"), (',', ','), ('np.nan', 'RB'), (',', ','), (\"'\", \"''\"), (\"'\", \"''\"), (',', ','), (\"'\", \"''\"), ('I', 'PRP'), ('love', 'VBP'), ('AI', 'NNP'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), (\"'\", \"''\"), (',', ','), ('``', '``'), (',', ','), (\"'Deep\", 'JJ'), ('Learning', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('subset', 'NN'), ('of', 'IN'), ('AI', 'NNP'), (\"'\", 'POS'), (',', ','), (\"'\", \"''\"), ('NLP', 'NNP'), ('applications', 'NNS'), ('are', 'VBP'), ('amazing', 'VBG'), (\"'\", \"''\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words(\"english\"))\n",
        "clean_tokens = [word for word in token_word if word.lower() not in stop_words]\n",
        "print(clean_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUqyu94VMzPu",
        "outputId": "b821a3aa-22a9-42f0-b6ee-8194b4d01c35"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'\", 'Natural', 'Language', 'Processing', 'fun', \"'\", ',', 'np.nan', ',', \"'\", \"'\", ',', \"'\", 'love', 'AI', 'machine', 'learning', \"'\", ',', '``', ',', \"'Deep\", 'Learning', 'subset', 'AI', \"'\", ',', \"'\", 'NLP', 'applications', 'amazing', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in clean_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aG_F-0ONDM5",
        "outputId": "b5549d89-3513-4b0a-a6da-92895ddbec80"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'\", 'natur', 'languag', 'process', 'fun', \"'\", ',', 'np.nan', ',', \"'\", \"'\", ',', \"'\", 'love', 'ai', 'machin', 'learn', \"'\", ',', '``', ',', \"'deep\", 'learn', 'subset', 'ai', \"'\", ',', \"'\", 'nlp', 'applic', 'amaz', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemme=WordNetLemmatizer()\n",
        "lemme_tokens = [lemme.lemmatize(word) for word in clean_tokens]\n",
        "print(lemme_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b18BsYYiNESZ",
        "outputId": "38954e31-9de5-4153-aab3-2ebb1fc2a8dc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'\", 'Natural', 'Language', 'Processing', 'fun', \"'\", ',', 'np.nan', ',', \"'\", \"'\", ',', \"'\", 'love', 'AI', 'machine', 'learning', \"'\", ',', '``', ',', \"'Deep\", 'Learning', 'subset', 'AI', \"'\", ',', \"'\", 'NLP', 'application', 'amazing', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = [document]\n",
        "vectornize = TfidfVectorizer(stop_words=\"english\")\n",
        "tfidf = vectornize.fit_transform(document)\n",
        "\n",
        "df_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectornize.get_feature_names_out())\n",
        "print(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmyI8WVGNFep",
        "outputId": "cfcf6fd6-382b-49c6-c854-6781f763c727"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         ai   amazing  applications      deep       fun  language  learning  \\\n",
            "0  0.436436  0.218218      0.218218  0.218218  0.218218  0.218218  0.436436   \n",
            "\n",
            "       love   machine       nan   natural       nlp        np  processing  \\\n",
            "0  0.218218  0.218218  0.218218  0.218218  0.218218  0.218218    0.218218   \n",
            "\n",
            "     subset  \n",
            "0  0.218218  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVD_dZZpNHX7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}